{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "9aHTrpANhSoC",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WAnqNAzfcVu0"
   },
   "source": [
    "# Emotion prediction with GoEmotions and PRADO\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OcNoWgG7hvIs"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/models/blob/master/research/seq_flow_lite/demo/colab/emotion_colab.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/models/blob/master/research/seq_flow_lite/demo/colab/emotion_colab.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dhekoIrWiSsv"
   },
   "source": [
    "In this tutorial, we will work through training a neural emotion prediction model, using the tensorflow-models PIP package, and Bazel.\n",
    "\n",
    "This tutorial is using GoEmotions, an emotion prediction dataset, available on [TensorFlow TFDS](https://www.tensorflow.org/datasets/catalog/goemotions). We will be training a sequence projection model architecture named PRADO, available on [TensorFlow Model Garden](https://github.com/tensorflow/models/blob/master/research/seq_flow_lite/models/prado.py). Finally, we will examine an application of emotion prediction to emoji suggestions from text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "grmac7ZYj02a"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4aGnloeD1Mfo"
   },
   "source": [
    "### Install Tensorflow 2.11.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MqP5qpAR1W4f"
   },
   "source": [
    "The seq_flow_lite library has been written with the assumption that tensorflow 2.11.0 will be used.  It may be necessary to restart the runtime after installing the correct version of Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "hzuq_GVn1nXO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.18.0-cp312-cp312-win_amd64.whl.metadata (3.3 kB)\n",
      "Collecting tensorflow-intel==2.18.0 (from tensorflow)\n",
      "  Downloading tensorflow_intel-2.18.0-cp312-cp312-win_amd64.whl.metadata (4.9 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading flatbuffers-25.1.24-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in c:\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (5.27.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading termcolor-2.5.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (4.12.2)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading wrapt-1.17.2-cp312-cp312-win_amd64.whl.metadata (6.5 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading grpcio-1.70.0-cp312-cp312-win_amd64.whl.metadata (4.0 kB)\n",
      "Collecting tensorboard<2.19,>=2.18 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading keras-3.8.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in c:\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.26.4)\n",
      "Collecting h5py>=3.11.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading h5py-3.12.1-cp312-cp312-win_amd64.whl.metadata (2.5 kB)\n",
      "Collecting ml-dtypes<0.5.0,>=0.4.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading ml_dtypes-0.4.1-cp312-cp312-win_amd64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\python312\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: rich in c:\\python312\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (13.8.1)\n",
      "Collecting namex (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting optree (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading optree-0.14.0-cp312-cp312-win_amd64.whl.metadata (48 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2024.8.30)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\python312\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\python312\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\python312\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\python312\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\python312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.1.2)\n",
      "Downloading tensorflow-2.18.0-cp312-cp312-win_amd64.whl (7.5 kB)\n",
      "Downloading tensorflow_intel-2.18.0-cp312-cp312-win_amd64.whl (390.3 MB)\n",
      "   ---------------------------------------- 0.0/390.3 MB ? eta -:--:--\n",
      "   - -------------------------------------- 13.4/390.3 MB 76.2 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 32.5/390.3 MB 89.8 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 50.6/390.3 MB 89.6 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 68.9/390.3 MB 89.8 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 90.2/390.3 MB 91.4 MB/s eta 0:00:04\n",
      "   ----------- --------------------------- 110.1/390.3 MB 92.5 MB/s eta 0:00:04\n",
      "   ------------- ------------------------- 130.3/390.3 MB 92.5 MB/s eta 0:00:03\n",
      "   --------------- ----------------------- 150.5/390.3 MB 92.4 MB/s eta 0:00:03\n",
      "   ---------------- ---------------------- 169.6/390.3 MB 91.9 MB/s eta 0:00:03\n",
      "   ------------------ -------------------- 186.9/390.3 MB 91.2 MB/s eta 0:00:03\n",
      "   -------------------- ------------------ 206.0/390.3 MB 91.5 MB/s eta 0:00:03\n",
      "   ---------------------- ---------------- 224.7/390.3 MB 91.5 MB/s eta 0:00:02\n",
      "   ------------------------ -------------- 242.0/390.3 MB 91.0 MB/s eta 0:00:02\n",
      "   ------------------------- ------------- 260.0/390.3 MB 90.9 MB/s eta 0:00:02\n",
      "   --------------------------- ----------- 275.5/390.3 MB 90.6 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 292.0/390.3 MB 89.1 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 303.0/390.3 MB 86.8 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 316.4/390.3 MB 84.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 332.9/390.3 MB 84.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 345.5/390.3 MB 82.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 361.2/390.3 MB 80.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 378.3/390.3 MB 80.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.1/390.3 MB 79.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.1/390.3 MB 79.4 MB/s eta 0:00:01\n",
      "   --------------------------------------- 390.3/390.3 MB 71.6 MB/s eta 0:00:00\n",
      "Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-25.1.24-py2.py3-none-any.whl (30 kB)\n",
      "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading grpcio-1.70.0-cp312-cp312-win_amd64.whl (4.3 MB)\n",
      "   ---------------------------------------- 0.0/4.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 4.3/4.3 MB 64.0 MB/s eta 0:00:00\n",
      "Downloading h5py-3.12.1-cp312-cp312-win_amd64.whl (3.0 MB)\n",
      "   ---------------------------------------- 0.0/3.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 3.0/3.0 MB 88.1 MB/s eta 0:00:00\n",
      "Downloading keras-3.8.0-py3-none-any.whl (1.3 MB)\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.3/1.3 MB 69.3 MB/s eta 0:00:00\n",
      "Downloading libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
      "   ---------------------------------------- 0.0/26.4 MB ? eta -:--:--\n",
      "   ------------------------- -------------- 17.0/26.4 MB 89.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 26.4/26.4 MB 83.8 MB/s eta 0:00:00\n",
      "Downloading ml_dtypes-0.4.1-cp312-cp312-win_amd64.whl (127 kB)\n",
      "Downloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Downloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
      "   ---------------------------------------- 0.0/5.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 5.5/5.5 MB 67.2 MB/s eta 0:00:00\n",
      "Downloading termcolor-2.5.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading wrapt-1.17.2-cp312-cp312-win_amd64.whl (38 kB)\n",
      "Downloading Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Downloading optree-0.14.0-cp312-cp312-win_amd64.whl (299 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, termcolor, tensorboard-data-server, optree, opt-einsum, ml-dtypes, markdown, h5py, grpcio, google-pasta, gast, astunparse, absl-py, tensorboard, keras, tensorflow-intel, tensorflow\n",
      "Successfully installed absl-py-2.1.0 astunparse-1.6.3 flatbuffers-25.1.24 gast-0.6.0 google-pasta-0.2.0 grpcio-1.70.0 h5py-3.12.1 keras-3.8.0 libclang-18.1.1 markdown-3.7 ml-dtypes-0.4.1 namex-0.0.8 opt-einsum-3.4.0 optree-0.14.0 tensorboard-2.18.0 tensorboard-data-server-0.7.2 tensorflow-2.18.0 tensorflow-intel-2.18.0 termcolor-2.5.0 wrapt-1.17.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ (C:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ (C:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ (C:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Python312\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D_mi4NZeeB1l"
   },
   "source": [
    "### Install the TensorFlow Datasets pip package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tCk46-HdmIyD"
   },
   "source": [
    "`tensorflow_datasets` is a set of collection of datasets that includes the GoEmotions dataset. We install it with pip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "mvO0_HcKx0_V"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_datasets\n",
      "  Downloading tensorflow_datasets-4.9.7-py3-none-any.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: absl-py in c:\\python312\\lib\\site-packages (from tensorflow_datasets) (2.1.0)\n",
      "Requirement already satisfied: click in c:\\python312\\lib\\site-packages (from tensorflow_datasets) (8.1.7)\n",
      "Collecting dm-tree (from tensorflow_datasets)\n",
      "  Downloading dm_tree-0.1.8-cp312-cp312-win_amd64.whl.metadata (2.0 kB)\n",
      "Collecting immutabledict (from tensorflow_datasets)\n",
      "  Downloading immutabledict-4.2.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: numpy in c:\\python312\\lib\\site-packages (from tensorflow_datasets) (1.26.4)\n",
      "Collecting promise (from tensorflow_datasets)\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: protobuf>=3.20 in c:\\python312\\lib\\site-packages (from tensorflow_datasets) (5.27.3)\n",
      "Requirement already satisfied: psutil in c:\\python312\\lib\\site-packages (from tensorflow_datasets) (6.1.0)\n",
      "Collecting pyarrow (from tensorflow_datasets)\n",
      "  Downloading pyarrow-19.0.0-cp312-cp312-win_amd64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\python312\\lib\\site-packages (from tensorflow_datasets) (2.32.3)\n",
      "Collecting simple-parsing (from tensorflow_datasets)\n",
      "  Downloading simple_parsing-0.1.7-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting tensorflow-metadata (from tensorflow_datasets)\n",
      "  Downloading tensorflow_metadata-1.16.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: termcolor in c:\\python312\\lib\\site-packages (from tensorflow_datasets) (2.5.0)\n",
      "Collecting toml (from tensorflow_datasets)\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: tqdm in c:\\python312\\lib\\site-packages (from tensorflow_datasets) (4.66.5)\n",
      "Requirement already satisfied: wrapt in c:\\python312\\lib\\site-packages (from tensorflow_datasets) (1.17.2)\n",
      "Collecting etils>=1.9.1 (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets)\n",
      "  Downloading etils-1.11.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: fsspec in c:\\python312\\lib\\site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (2024.9.0)\n",
      "Requirement already satisfied: importlib_resources in c:\\python312\\lib\\site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (6.4.5)\n",
      "Requirement already satisfied: typing_extensions in c:\\python312\\lib\\site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (4.12.2)\n",
      "Collecting zipp (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets)\n",
      "  Downloading zipp-3.21.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python312\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python312\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python312\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python312\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\python312\\lib\\site-packages (from click->tensorflow_datasets) (0.4.6)\n",
      "Requirement already satisfied: six in c:\\python312\\lib\\site-packages (from promise->tensorflow_datasets) (1.16.0)\n",
      "Requirement already satisfied: docstring-parser<1.0,>=0.15 in c:\\python312\\lib\\site-packages (from simple-parsing->tensorflow_datasets) (0.16)\n",
      "Collecting googleapis-common-protos<2,>=1.56.4 (from tensorflow-metadata->tensorflow_datasets)\n",
      "  Downloading googleapis_common_protos-1.66.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Downloading tensorflow_datasets-4.9.7-py3-none-any.whl (5.3 MB)\n",
      "   ---------------------------------------- 0.0/5.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 5.3/5.3 MB 46.2 MB/s eta 0:00:00\n",
      "Downloading etils-1.11.0-py3-none-any.whl (165 kB)\n",
      "Downloading dm_tree-0.1.8-cp312-cp312-win_amd64.whl (101 kB)\n",
      "Downloading immutabledict-4.2.1-py3-none-any.whl (4.7 kB)\n",
      "Downloading pyarrow-19.0.0-cp312-cp312-win_amd64.whl (25.2 MB)\n",
      "   ---------------------------------------- 0.0/25.2 MB ? eta -:--:--\n",
      "   ----------------- ---------------------- 11.0/25.2 MB 52.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.2/25.2 MB 66.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 25.2/25.2 MB 59.2 MB/s eta 0:00:00\n",
      "Downloading simple_parsing-0.1.7-py3-none-any.whl (112 kB)\n",
      "Downloading tensorflow_metadata-1.16.1-py3-none-any.whl (28 kB)\n",
      "Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Downloading googleapis_common_protos-1.66.0-py2.py3-none-any.whl (221 kB)\n",
      "Downloading zipp-3.21.0-py3-none-any.whl (9.6 kB)\n",
      "Building wheels for collected packages: promise\n",
      "  Building wheel for promise (setup.py): started\n",
      "  Building wheel for promise (setup.py): finished with status 'done'\n",
      "  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21544 sha256=6ad0672e73820693726654be61cd02ce05916c391b5c4a4ae3bcc86d570d2c79\n",
      "  Stored in directory: c:\\users\\leeha\\appdata\\local\\pip\\cache\\wheels\\e7\\e6\\28\\864bdfee5339dbd6ddcb5a186286a8e217648ec198bdf0097d\n",
      "Successfully built promise\n",
      "Installing collected packages: dm-tree, zipp, toml, simple-parsing, pyarrow, promise, immutabledict, googleapis-common-protos, etils, tensorflow-metadata, tensorflow_datasets\n",
      "Successfully installed dm-tree-0.1.8 etils-1.11.0 googleapis-common-protos-1.66.0 immutabledict-4.2.1 promise-2.3 pyarrow-19.0.0 simple-parsing-0.1.7 tensorflow-metadata-1.16.1 tensorflow_datasets-4.9.7 toml-0.10.2 zipp-3.21.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ (C:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ (C:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ (C:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Python312\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p2wqyg-7mbfV"
   },
   "source": [
    "### Install the Sequence Projection Models package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1JRZS_aSeINK"
   },
   "source": [
    "Install Bazel: This will allow us to build custom TensorFlow ops used by the PRADO architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "N00X4P229Ppm"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sudo' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'sudo' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'sudo' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'sudo' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!sudo apt install curl gnupg\n",
    "!curl https://bazel.build/bazel-release.pub.gpg | sudo apt-key add -\n",
    "!echo \"deb [arch=amd64] https://storage.googleapis.com/bazel-apt stable jdk1.8\" | sudo tee /etc/apt/sources.list.d/bazel.list\n",
    "!sudo apt update\n",
    "!sudo apt install bazel=5.4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9JeSDpZFelL5"
   },
   "source": [
    "Install the library:\n",
    "* `seq_flow_lite` includes the PRADO architecture and custom ops.\n",
    "* We download the code from GitHub, and then build and install the TF and TFLite ops used by the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mktlCYcd9iLG"
   },
   "outputs": [],
   "source": [
    "!git clone https://www.github.com/tensorflow/models\n",
    "!models/research/seq_flow_lite/demo/colab/setup_workspace.sh\n",
    "!pip install models/research/seq_flow_lite\n",
    "!rm -rf models/research/seq_flow_lite/tf_ops\n",
    "!rm -rf models/research/seq_flow_lite/tflite_ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rP8iKa4Il4mL"
   },
   "source": [
    "## Training an Emotion Prediction Model\n",
    "\n",
    "* First, we load the GoEmotions data from TFDS.\n",
    "* Next, we prepare the PRADO model for training. We set up the model configuration, including hyperparameters and labels. We also prepare the dataset, which involves projecting the inputs from the dataset, and passing the projections to the model.  This is needed because a model training on TPU can not handle string inputs.\n",
    "* Finally, we train and evaluate the model and produce model-level and per-label metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YtvR40K8K0Bn"
   },
   "source": [
    "***Start here on Runtime reset***, once the packages above are properly installed:\n",
    "* Go to the `seq_flow_lite` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ImEejssVKvxR"
   },
   "outputs": [],
   "source": [
    "%cd models/research/seq_flow_lite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uwSPqHXAeQ6H"
   },
   "source": [
    "* Import the Tensorflow and Tensorflow Dataset libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kc4y4n80eL_b"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j-CtG3cagPgl"
   },
   "source": [
    "### The data: GoEmotions\n",
    "In this tutorial, we use the [GoEmotions dataset from TFDS](https://www.tensorflow.org/datasets/catalog/goemotions).\n",
    "\n",
    "GoEmotions is a corpus of comments extracted from Reddit, with human annotations to 27 emotion categories or Neutral.\n",
    "\n",
    "*   Number of labels: 27.\n",
    "*   Size of training dataset: 43,410.\n",
    "*   Size of evaluation dataset: 5,427.\n",
    "*   Maximum sequence length in training and evaluation datasets: 30.\n",
    "\n",
    "The emotion categories are admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bvsn_s3S0SAt"
   },
   "source": [
    "Load the data from TFDS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KtTLwtEqwcR2"
   },
   "outputs": [],
   "source": [
    "ds = tfds.load('goemotions', split='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gJuu4jKet9zq"
   },
   "source": [
    "Print 5 sample data elements from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y0O18rSLuDx5"
   },
   "outputs": [],
   "source": [
    "for element in ds.take(5):\n",
    "  print(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UAz-tdQfuVBn"
   },
   "source": [
    "### The model: PRADO\n",
    "\n",
    "We train an Emotion Prediction model, based on the [PRADO architecture](https://github.com/tensorflow/models/blob/master/research/seq_flow_lite/models/prado.py) from the [Sequence Projection Models package](https://github.com/tensorflow/models/tree/master/research/seq_flow_lite).\n",
    "\n",
    "PRADO projects input sequences to fixed sized features. The idea behind this approach is to build embedding-free models that minimize the model size. Instead of using an embedding table to lookup embeddings, sequence projection models compute them on the fly, resulting in space-efficient models.\n",
    "\n",
    "In this section, we prepare the PRADO model for training.\n",
    "\n",
    "This GoEmotions dataset is not set up so that it can be directly fed into the PRADO model, so below, we also handle the necessary preprocessing by providing a dataset builder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e9uPSZYpgBqP"
   },
   "source": [
    "Prepare the model configuration:\n",
    "* Enumerate the labels expected to be found in the GoEmotions dataset.\n",
    "* Prepare the `MODEL_CONFIG` dictionary which includes training parameters for the model. See sample configs for the PRADO model [here](https://github.com/tensorflow/models/tree/master/research/seq_flow_lite/configs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DkQMnTcLyFeR"
   },
   "outputs": [],
   "source": [
    "LABELS = [\n",
    "    'admiration',\n",
    "    'amusement',\n",
    "    'anger',\n",
    "    'annoyance',\n",
    "    'approval',\n",
    "    'caring',\n",
    "    'confusion',\n",
    "    'curiosity',\n",
    "    'desire',\n",
    "    'disappointment',\n",
    "    'disapproval',\n",
    "    'disgust',\n",
    "    'embarrassment',\n",
    "    'excitement',\n",
    "    'fear',\n",
    "    'gratitude',\n",
    "    'grief',\n",
    "    'joy',\n",
    "    'love',\n",
    "    'nervousness',\n",
    "    'optimism',\n",
    "    'pride',\n",
    "    'realization',\n",
    "    'relief',\n",
    "    'remorse',\n",
    "    'sadness',\n",
    "    'surprise',\n",
    "    'neutral',\n",
    "]\n",
    "\n",
    "# Model training parameters.\n",
    "CONFIG = {\n",
    "    'name': 'models.prado',\n",
    "    'batch_size': 1024,\n",
    "    'train_steps': 10000,\n",
    "    'learning_rate': 0.0006,\n",
    "    'learning_rate_decay_steps': 340,\n",
    "    'learning_rate_decay_rate': 0.7,\n",
    "}\n",
    "\n",
    "# Limits the amount of logging output produced by the training run, in order to\n",
    "# avoid browser slowdowns.\n",
    "CONFIG['save_checkpoints_steps'] = int(CONFIG['train_steps'] / 10)\n",
    "\n",
    "MODEL_CONFIG = {\n",
    "    'labels': LABELS,\n",
    "    'multilabel': True,\n",
    "    'quantize': False,\n",
    "    'max_seq_len': 128,\n",
    "    'max_seq_len_inference': 128,\n",
    "    'exclude_nonalphaspace_unicodes': False,\n",
    "    'split_on_space': True,\n",
    "    'embedding_regularizer_scale': 0.035,\n",
    "    'embedding_size': 64,\n",
    "    'bigram_channels': 64,\n",
    "    'trigram_channels': 64,\n",
    "    'feature_size': 512,\n",
    "    'network_regularizer_scale': 0.0001,\n",
    "    'keep_prob': 0.5,\n",
    "    'word_novelty_bits': 0,\n",
    "    'doc_size_levels': 0,\n",
    "    'add_bos_tag': False,\n",
    "    'add_eos_tag': False,\n",
    "    'pre_logits_fc_layers': [],\n",
    "    'text_distortion_probability': 0.0,\n",
    "}\n",
    "\n",
    "CONFIG['model_config'] = MODEL_CONFIG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R-pUW649gfzA"
   },
   "source": [
    "Write a function that builds the datasets for the model.  It will load the data, handle batching, and generate projections for the input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "unYlUYXq119f"
   },
   "outputs": [],
   "source": [
    "from layers import base_layers\n",
    "from layers import projection_layers\n",
    "\n",
    "def build_dataset(mode, inspect=False):\n",
    "  if mode == base_layers.TRAIN:\n",
    "    split = 'train'\n",
    "    count = None\n",
    "  elif mode == base_layers.EVAL:\n",
    "    split = 'test'\n",
    "    count = 1\n",
    "  else:\n",
    "    raise ValueError('mode={}, must be TRAIN or EVAL'.format(mode))\n",
    "\n",
    "  batch_size = CONFIG['batch_size']\n",
    "  if inspect:\n",
    "    batch_size = 1\n",
    "\n",
    "  # Convert examples from their dataset format into the model format.\n",
    "  def process_input(features):\n",
    "    # Generate the projection for each comment_text input.  The final tensor \n",
    "    # will have the shape [batch_size, number of tokens, feature size].\n",
    "    # Additionally, we generate a tensor containing the number of tokens for\n",
    "    # each comment_text (seq_length).  This is needed because the projection\n",
    "    # tensor is a full tensor, and we are not using EOS tokens.\n",
    "    text = features['comment_text']\n",
    "    text = tf.reshape(text, [batch_size])\n",
    "    projection_layer = projection_layers.ProjectionLayer(MODEL_CONFIG, mode)\n",
    "    projection, seq_length = projection_layer(text)\n",
    "\n",
    "    # Convert the labels into an indicator tensor, using the LABELS indices.\n",
    "    label = tf.stack([features[label] for label in LABELS], axis=-1)\n",
    "    label = tf.cast(label, tf.float32)\n",
    "    label = tf.reshape(label, [batch_size, len(LABELS)])\n",
    "\n",
    "    model_features = ({'projection': projection, 'sequence_length': seq_length}, label)\n",
    "\n",
    "    if inspect:\n",
    "      model_features = (model_features[0], model_features[1], features)\n",
    "\n",
    "    return model_features\n",
    "\n",
    "  ds = tfds.load('goemotions', split=split)\n",
    "  ds = ds.repeat(count=count)\n",
    "  ds = ds.shuffle(buffer_size=batch_size * 2)\n",
    "  ds = ds.batch(batch_size, drop_remainder=True)\n",
    "  ds = ds.map(process_input,\n",
    "              num_parallel_calls=tf.data.experimental.AUTOTUNE,\n",
    "              deterministic=False)\n",
    "  ds = ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "  return ds\n",
    "\n",
    "train_dataset = build_dataset(base_layers.TRAIN)\n",
    "test_dataset = build_dataset(base_layers.EVAL)\n",
    "inspect_dataset = build_dataset(base_layers.TRAIN, inspect=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DQmYWg6ivCHS"
   },
   "source": [
    "Print a batch of examples in model format.  This will consist of:\n",
    "* the projection tensors (projection and seq_length)\n",
    "* the label tensor (second tuple value)\n",
    "\n",
    "The projection tensor is a **[batch size, max_seq_length, feature_size]** floating point tensor.  The **[b, i]** vector is a feature vector of the **i**th token of the **b**th comment_text.  The rest of the tensor is zero-padded, and the\n",
    "seq_length tensor indicates the number of features vectors for each comment_text.\n",
    "\n",
    "The label tensor is an indicator tensor of the set of true labels for the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1OyK7rjTvBjF"
   },
   "outputs": [],
   "source": [
    "example = next(iter(train_dataset))\n",
    "print(\"inputs = {}\".format(example[0]))\n",
    "print(\"labels = {}\".format(example[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ytMQHT5Kd7A_"
   },
   "source": [
    "In this version of the dataset, the original example has been added as the third element of the tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "29EzRoCfI91r"
   },
   "outputs": [],
   "source": [
    "example = next(iter(inspect_dataset))\n",
    "print(\"inputs = {}\".format(example[0]))\n",
    "print(\"labels = {}\".format(example[1]))\n",
    "print(\"original example = {}\".format(example[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CLDbHTIvvX11"
   },
   "source": [
    "### Train and Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QqUTa7wXsHoO"
   },
   "source": [
    "First we define a function to build the model.  We vary the model inputs depending on task.  For training and evaluation, we'll take the projection and sequence length as inputs.  Otherwise, we'll take strings as inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "erEiNX3ToLZ1"
   },
   "outputs": [],
   "source": [
    "from models import prado\n",
    "\n",
    "def build_model(mode):\n",
    "  # First we define our inputs.\n",
    "  inputs = []\n",
    "  if mode == base_layers.TRAIN or mode == base_layers.EVAL:\n",
    "    # For TRAIN and EVAL, we'll be getting dataset examples,\n",
    "    # so we'll get projections and sequence_lengths.\n",
    "    projection = tf.keras.Input(\n",
    "        shape=(MODEL_CONFIG['max_seq_len'], MODEL_CONFIG['feature_size']),\n",
    "        name='projection',\n",
    "        dtype='float32')\n",
    "\n",
    "    sequence_length = tf.keras.Input(\n",
    "        shape=(), name='sequence_length', dtype='float32')\n",
    "    inputs = [projection, sequence_length]\n",
    "  else:\n",
    "    # Otherwise, we get string inputs which we need to project.\n",
    "    input = tf.keras.Input(shape=(), name='input', dtype='string')\n",
    "    projection_layer = projection_layers.ProjectionLayer(MODEL_CONFIG, mode)\n",
    "    projection, sequence_length = projection_layer(input)\n",
    "    inputs = [input]\n",
    "\n",
    "  # Next we add the model layer.\n",
    "  model_layer = prado.Encoder(MODEL_CONFIG, mode)\n",
    "  logits = model_layer(projection, sequence_length)\n",
    "\n",
    "  # Finally we add an activation layer.\n",
    "  if MODEL_CONFIG['multilabel']:\n",
    "    activation = tf.keras.layers.Activation('sigmoid', name='predictions')\n",
    "  else:\n",
    "    activation = tf.keras.layers.Activation('softmax', name='predictions')\n",
    "  predictions = activation(logits)\n",
    "\n",
    "  model = tf.keras.Model(\n",
    "      inputs=inputs,\n",
    "      outputs=[predictions])\n",
    "  \n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "caHpK9Htv40g"
   },
   "source": [
    "Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2xM-2R38kogo"
   },
   "outputs": [],
   "source": [
    "# Remove any previous training data.\n",
    "!rm -rf model\n",
    "\n",
    "model = build_model(base_layers.TRAIN)\n",
    "\n",
    "# Create the optimizer.\n",
    "learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=CONFIG['learning_rate'],\n",
    "    decay_rate=CONFIG['learning_rate_decay_rate'],\n",
    "    decay_steps=CONFIG['learning_rate_decay_steps'],\n",
    "    staircase=True)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Define the loss function.\n",
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss)\n",
    "\n",
    "epochs = int(CONFIG['train_steps'] / CONFIG['save_checkpoints_steps'])\n",
    "model.fit(\n",
    "    x=train_dataset,\n",
    "    epochs=epochs,\n",
    "    validation_data=test_dataset,\n",
    "    steps_per_epoch=CONFIG['save_checkpoints_steps'])\n",
    "\n",
    "model.save_weights('model/model_checkpoint')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0hdbXBs0g3oX"
   },
   "source": [
    "Load a training checkpoint and evaluate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A1qc9GNtF3s5"
   },
   "outputs": [],
   "source": [
    "model = build_model(base_layers.EVAL)\n",
    "\n",
    "# Define metrics over each category.\n",
    "metrics = []\n",
    "for i, label in enumerate(LABELS):\n",
    "  metric = tf.keras.metrics.Precision(\n",
    "      thresholds=[0.5],\n",
    "      class_id=i,\n",
    "      name='precision@0.5/{}'.format(label))\n",
    "  metrics.append(metric)\n",
    "  metric = tf.keras.metrics.Recall(\n",
    "      thresholds=[0.5],\n",
    "      class_id=i,\n",
    "      name='recall@0.5/{}'.format(label))\n",
    "  metrics.append(metric)\n",
    "\n",
    "# Define metrics over the entire task.\n",
    "metric = tf.keras.metrics.Precision(thresholds=[0.5], name='precision@0.5/all')\n",
    "metrics.append(metric)\n",
    "metric = tf.keras.metrics.Recall(thresholds=[0.5], name='recall@0.5/all')\n",
    "metrics.append(metric)\n",
    "\n",
    "model.compile(metrics=metrics)\n",
    "model.load_weights('model/model_checkpoint')\n",
    "result = model.evaluate(x=test_dataset, return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Namwa3enwQBc"
   },
   "source": [
    "Print evaluation metrics for the model, as well as per emotion label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l420PosisfXN"
   },
   "outputs": [],
   "source": [
    "for label in LABELS:\n",
    "  precision_key = 'precision@0.5/{}'.format(label)\n",
    "  recall_key = 'recall@0.5/{}'.format(label)\n",
    "  if precision_key in result and recall_key in result:\n",
    "    print('{}: (precision@0.5: {}, recall@0.5: {})'.format(\n",
    "        label, result[precision_key], result[recall_key]))\n",
    "    \n",
    "precision_key = 'precision@0.5/all'\n",
    "recall_key = 'recall@0.5/all'\n",
    "if precision_key in result and recall_key in result:\n",
    "  print('all: (precision@0.5: {}, recall@0.5: {})'.format(\n",
    "      result[precision_key], result[recall_key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AZSWnwTMqZ5f"
   },
   "source": [
    "## Suggest Emojis using an Emotion Prediction model\n",
    "\n",
    "In this section, we apply the Emotion Prediction model trained above to suggest emojis relevant to input text.\n",
    "\n",
    "Refer to our [GoEmotions Model Card](https://github.com/google-research/google-research/blob/master/goemotions/goemotions_model_card.pdf) for additional uses of the model and considerations and limitations for using the GoEmotions data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aybpGQV1qr8I"
   },
   "source": [
    "Map each emotion label to a relevant emoji:\n",
    "* Emotions are subtle and multi-faceted. In many cases, no one emoji can truely capture the full complexity of the human experience behind each emotion. \n",
    "* For the purpose of this exercise, we will select an emoji that captures at least one facet that is conveyed by an emotion label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lgs12b90qmSQ"
   },
   "outputs": [],
   "source": [
    "EMOJI_MAP = {\n",
    "    'admiration': '👏',\n",
    "    'amusement': '😂',\n",
    "    'anger': '😡',\n",
    "    'annoyance': '😒',\n",
    "    'approval': '👍',\n",
    "    'caring': '🤗',\n",
    "    'confusion': '😕',\n",
    "    'curiosity': '🤔',\n",
    "    'desire': '😍',\n",
    "    'disappointment': '😞',\n",
    "    'disapproval': '👎',\n",
    "    'disgust': '🤮',\n",
    "    'embarrassment': '😳',\n",
    "    'excitement': '🤩',\n",
    "    'fear': '😨',\n",
    "    'gratitude': '🙏',\n",
    "    'grief': '😢',\n",
    "    'joy': '😃',\n",
    "    'love': '❤️',\n",
    "    'nervousness': '😬',\n",
    "    'optimism': '🤞',\n",
    "    'pride': '😌',\n",
    "    'realization': '💡',\n",
    "    'relief': '😅',\n",
    "    'remorse': '',\n",
    "    'sadness': '😞',\n",
    "    'surprise': '😲',\n",
    "    'neutral': '',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rh_3y7OL7JG_"
   },
   "source": [
    "Select sample inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rdD6xPpn7Mjm"
   },
   "outputs": [],
   "source": [
    "PREDICT_TEXT = [\n",
    "  b'Good for you!',\n",
    "  b'Happy birthday!',\n",
    "  b'I love you.',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vavivya6hGw0"
   },
   "source": [
    "Run inference for the selected examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tJ6iyLlLo5-3"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "model = build_model(base_layers.PREDICT)\n",
    "model.load_weights('model/model_checkpoint')\n",
    "\n",
    "for text in PREDICT_TEXT:\n",
    "  results = model.predict(x=[text])\n",
    "  print('')\n",
    "  print('{}:'.format(text))\n",
    "  labels = np.flip(np.argsort(results[0]))\n",
    "  for x in range(3):\n",
    "    label = LABELS[labels[x]]\n",
    "    label = EMOJI_MAP[label] if EMOJI_MAP[label] else label\n",
    "    print('{}: {}'.format(label, results[0][labels[x]]))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Emotion prediction with GoEmotions and PRADO",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
